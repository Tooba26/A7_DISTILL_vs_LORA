{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)\n",
    "\n",
    "In this lecture, we will explore the architecture of DistilBERT, its key components, and how it can be utilized for various natural language processing tasks. Additionally, we'll discuss its advantages, limitations, and provide hands-on examples to showcase its effectiveness.\n",
    "\n",
    "Reference : [The Theory](https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a) | [Code](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('3.4.1', '4.35.2', '2.6.0+cu124')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install datasets --upgrade\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "datasets.__version__, transformers.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading our MNLI part of the GLUE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'id'],\n",
       "        num_rows: 43410\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'id'],\n",
       "        num_rows: 5426\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'id'],\n",
       "        num_rows: 5427\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "raw_datasets = datasets.load_dataset('google-research-datasets/go_emotions')\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'admiration': 0, 'amusement': 1, 'anger': 2, 'annoyance': 3, 'approval': 4, 'caring': 5, 'confusion': 6, 'curiosity': 7, 'desire': 8, 'disappointment': 9, 'disapproval': 10, 'disgust': 11, 'embarrassment': 12, 'excitement': 13, 'fear': 14, 'gratitude': 15, 'grief': 16, 'joy': 17, 'love': 18, 'nervousness': 19, 'optimism': 20, 'pride': 21, 'realization': 22, 'relief': 23, 'remorse': 24, 'sadness': 25, 'surprise': 26, 'neutral': 27}\n"
     ]
    }
   ],
   "source": [
    "label_list = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\",\n",
    "    \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\",\n",
    "    \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\",\n",
    "    \"nervousness\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\",\n",
    "    \"surprise\", \"neutral\"\n",
    "]\n",
    "\n",
    "# Map to label2id\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'labels': Sequence(feature=ClassLabel(names=['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'], id=None), length=-1, id=None), 'id': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'admiration',\n",
       " 1: 'amusement',\n",
       " 2: 'anger',\n",
       " 3: 'annoyance',\n",
       " 4: 'approval',\n",
       " 5: 'caring',\n",
       " 6: 'confusion',\n",
       " 7: 'curiosity',\n",
       " 8: 'desire',\n",
       " 9: 'disappointment',\n",
       " 10: 'disapproval',\n",
       " 11: 'disgust',\n",
       " 12: 'embarrassment',\n",
       " 13: 'excitement',\n",
       " 14: 'fear',\n",
       " 15: 'gratitude',\n",
       " 16: 'grief',\n",
       " 17: 'joy',\n",
       " 18: 'love',\n",
       " 19: 'nervousness',\n",
       " 20: 'optimism',\n",
       " 21: 'pride',\n",
       " 22: 'realization',\n",
       " 23: 'relief',\n",
       " 24: 'remorse',\n",
       " 25: 'sadness',\n",
       " 26: 'surprise',\n",
       " 27: 'neutral'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {i: v for v, i in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "# Convert the nested list to a flat NumPy array\n",
    "all_labels = np.concatenate(raw_datasets['train']['labels'])\n",
    "\n",
    "# Get the number of unique labels\n",
    "num_labels = np.unique(all_labels).size\n",
    "print(num_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"figures/BERT_embed.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "teacher_id = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_id)\n",
    "\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_id, \n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    ")\n",
    "\n",
    "teacher_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenize function\n",
    "# Tokenize and process labels\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples['text'], max_length=128, truncation=True)\n",
    "    labels_binary = np.zeros((len(examples['text']), num_labels), dtype=int)\n",
    "    for i, label_list in enumerate(examples['labels']):\n",
    "        for label in label_list:\n",
    "            labels_binary[i, label] = 1\n",
    "    result['labels'] = labels_binary.tolist()\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'id', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n",
      "{'text': \"My favourite food is anything I didn't have to cook myself.\", 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'id': 'eebbqej', 'input_ids': [101, 2026, 8837, 2833, 2003, 2505, 1045, 2134, 1005, 1056, 2031, 2000, 5660, 2870, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Inspect the result\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n"
     ]
    }
   ],
   "source": [
    "# list(task_to_keys[task_name])\n",
    "# Define task_to_keys for GoEmotions (single-text task)\n",
    "task_to_keys = {\n",
    "    'go_emotions': ('text', None)  # Single input column, no second sentence\n",
    "}\n",
    "\n",
    "# Set the task name\n",
    "task_name = 'go_emotions'\n",
    "\n",
    "# Get the columns, filtering out None\n",
    "column_dataset = [item for item in task_to_keys[task_name] if item is not None]\n",
    "print(column_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 43410\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5426\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5427\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove column : 'premise', 'hypothesis', 'idx'\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(column_dataset + [\"id\"])\n",
    "#rename column : 'labels'\n",
    "# tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2026, 8837, 2833, 2003, 2505, 1045, 2134, 1005, 1056, 2031, 2000,\n",
       "        5660, 2870, 1012,  102])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] my favourite food is anything i didn't have to cook myself. [SEP]\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='pt')#Data collator that will dynamically pad the inputs received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming tokenized_datasets is already created (from your previous tokenization step)\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=1150).select(range(40000))\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=1150).select(range(1000))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=1150).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    small_train_dataset, shuffle=True, batch_size=32, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(\n",
    "    small_test_dataset, batch_size=32, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(\n",
    "    small_eval_dataset, batch_size=32, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 28]) torch.Size([8, 26]) torch.Size([8, 26])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='pt')\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(small_train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "# Test the batch\n",
    "for batch in train_dataloader:\n",
    "    print(batch['labels'].shape, batch['input_ids'].shape, batch['attention_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model and losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Teacher Model & Student Model\n",
    "\n",
    "####  Architecture \n",
    "In the present work, the student - DistilBERT - has the same general architecture as BERT. \n",
    "- The `token-type embeddings` and the `pooler` are removed while `the number of layers` is reduced by a factor of 2. \n",
    "- Most of the operations used in the Transformer architecture `linear layer` and `layer normalisation` are highly optimized in modern linear algebra frameworks.\n",
    "- our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. \n",
    "- Thus we focus on reducing the number of layers.\n",
    "\n",
    "#### Initialize Student Model\n",
    "- To initialize a new model from an existing one, we need to access the weights of the old model (the teacher). \n",
    "- In order to get the weights, we first have to know how to access them. Weâ€™ll use BERT as our teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"admiration\",\n",
       "    \"1\": \"amusement\",\n",
       "    \"2\": \"anger\",\n",
       "    \"3\": \"annoyance\",\n",
       "    \"4\": \"approval\",\n",
       "    \"5\": \"caring\",\n",
       "    \"6\": \"confusion\",\n",
       "    \"7\": \"curiosity\",\n",
       "    \"8\": \"desire\",\n",
       "    \"9\": \"disappointment\",\n",
       "    \"10\": \"disapproval\",\n",
       "    \"11\": \"disgust\",\n",
       "    \"12\": \"embarrassment\",\n",
       "    \"13\": \"excitement\",\n",
       "    \"14\": \"fear\",\n",
       "    \"15\": \"gratitude\",\n",
       "    \"16\": \"grief\",\n",
       "    \"17\": \"joy\",\n",
       "    \"18\": \"love\",\n",
       "    \"19\": \"nervousness\",\n",
       "    \"20\": \"optimism\",\n",
       "    \"21\": \"pride\",\n",
       "    \"22\": \"realization\",\n",
       "    \"23\": \"relief\",\n",
       "    \"24\": \"remorse\",\n",
       "    \"25\": \"sadness\",\n",
       "    \"26\": \"surprise\",\n",
       "    \"27\": \"neutral\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"admiration\": 0,\n",
       "    \"amusement\": 1,\n",
       "    \"anger\": 2,\n",
       "    \"annoyance\": 3,\n",
       "    \"approval\": 4,\n",
       "    \"caring\": 5,\n",
       "    \"confusion\": 6,\n",
       "    \"curiosity\": 7,\n",
       "    \"desire\": 8,\n",
       "    \"disappointment\": 9,\n",
       "    \"disapproval\": 10,\n",
       "    \"disgust\": 11,\n",
       "    \"embarrassment\": 12,\n",
       "    \"excitement\": 13,\n",
       "    \"fear\": 14,\n",
       "    \"gratitude\": 15,\n",
       "    \"grief\": 16,\n",
       "    \"joy\": 17,\n",
       "    \"love\": 18,\n",
       "    \"nervousness\": 19,\n",
       "    \"neutral\": 27,\n",
       "    \"optimism\": 20,\n",
       "    \"pride\": 21,\n",
       "    \"realization\": 22,\n",
       "    \"relief\": 23,\n",
       "    \"remorse\": 24,\n",
       "    \"sadness\": 25,\n",
       "    \"surprise\": 26\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "- The student model has the same configuration, except the number of layers is reduced by a factor of 2\n",
    "- The student layers are initilized by copying one out of two layers of the teacher, starting with layer 0.\n",
    "- The head of the teacher is also copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertConfig\n",
    "# Get teacher configuration as a dictionnary\n",
    "configuration = teacher_model.config.to_dict()\n",
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half the number of hidden layer\n",
    "configuration['num_hidden_layers'] //= 2\n",
    "# Convert the dictionnary to the student configuration\n",
    "configuration = BertConfig.from_dict(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create uninitialized student model\n",
    "model = type(teacher_model)(configuration)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recursively copies the weights of the (teacher) to the (student).\n",
    "- This function is meant to be first called on a BertFor... model, but is then called on every children of that model recursively.\n",
    "- The only part that's not fully copied is the encoder, of which only half is copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertEncoder, BertModel\n",
    "from torch.nn import Module\n",
    "def distill_bert_weights(\n",
    "    teacher: Module,\n",
    "    student: Module,\n",
    "    layer_strategy: str = 'even'  # 'odd' or 'even'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively copies weights from the teacher to the student, selecting specific layers.\n",
    "    Args:\n",
    "        teacher: The teacher model (e.g., 12-layer BERT).\n",
    "        student: The student model (e.g., 6-layer BERT).\n",
    "        layer_strategy: 'odd' (layers 1, 3, 5, 7, 9, 11) or 'even' (layers 2, 4, 6, 8, 10, 12).\n",
    "    \"\"\"\n",
    "    # If the part is an entire BERT model, unpack and iterate\n",
    "    if isinstance(teacher, BertModel) or type(teacher).__name__.startswith('BertFor'):\n",
    "        for teacher_part, student_part in zip(teacher.children(), student.children()):\n",
    "            distill_bert_weights(teacher_part, student_part, layer_strategy)\n",
    "    # If the part is an encoder, copy the selected layers\n",
    "    elif isinstance(teacher, BertEncoder):\n",
    "        teacher_encoding_layers = [layer for layer in next(teacher.children())]  # 12 layers\n",
    "        student_encoding_layers = [layer for layer in next(student.children())]  # 6 layers\n",
    "        \n",
    "        # Define layer indices based on strategy (0-indexed)\n",
    "        if layer_strategy == 'odd':\n",
    "            teacher_layer_indices = [0, 2, 4, 6, 8, 10]  # Layers 1, 3, 5, 7, 9, 11\n",
    "        elif layer_strategy == 'even':\n",
    "            teacher_layer_indices = [1, 3, 5, 7, 9, 11]  # Layers 2, 4, 6, 8, 10, 12\n",
    "        else:\n",
    "            raise ValueError(\"layer_strategy must be 'odd' or 'even'\")\n",
    "        \n",
    "        # Copy weights from selected teacher layers to student layers\n",
    "        for student_idx, teacher_idx in enumerate(teacher_layer_indices):\n",
    "            student_encoding_layers[student_idx].load_state_dict(teacher_encoding_layers[teacher_idx].state_dict())\n",
    "    # Else the part is a head or something else, copy the state_dict\n",
    "    else:\n",
    "        student.load_state_dict(teacher.state_dict())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = distill_bert_weights(teacher=teacher_model, student=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher parameters : 109503772\n",
      "Student parameters : 66976540\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Teacher parameters :', count_parameters(teacher_model))\n",
    "print('Student parameters :', count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.16368301906532"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)/count_parameters(teacher_model) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It has 40% less parameters than bert-base-uncased"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "$$\n",
    "P_i(\\mathbf{z}_i, T) = \\frac{\\exp(\\mathbf{z}_i / T)}{\\sum_{q=0}^k \\exp(\\mathbf{z}_q / T)}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge Distillation\n",
    "\n",
    "#### CE Loss\n",
    "$$\\mathcal{L}_\\text{CE} = -\\sum^N_{j=0}\\sum_{i=0}^k {y}_i^{(j)}\\log(P_i({v}_i^{(j)}, 1))$$\n",
    "\n",
    "#### KL Loss\n",
    "$$\\mathcal{L}_\\text{KD} = -\\sum^N_{j=0}\\sum_{i=0}^k P_i({z}_i^{(j)}, T) \\log (P_i({v}_i^{(j)}, T))$$\n",
    "\n",
    "#### Cosine Embedding Loss\n",
    "$$\\mathcal{L}_{\\text{cosine}}(x_1, x_2, y) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(1 - y_i \\cdot \\cos(\\theta_i)\\right)$$\n",
    "\n",
    "<!-- $$\\mathcal{L} = \\lambda \\mathcal{L}_\\text{KD} + (1-\\lambda)\\mathcal{L}_\\text{CE}$$\n",
    " -->\n",
    "\n",
    "#### Total Loss\n",
    "$$\\mathcal{L} = \\mathcal{L}_\\text{KD} + \\mathcal{L}_\\text{CE} + \\mathcal{L}_{\\text{cosine}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DistillKL(nn.Module):\n",
    "    \"\"\"\n",
    "    Distilling the Knowledge in a Neural Network\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "\n",
    "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
    "    and student expects the input tensor to be log probabilities! \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistillKL, self).__init__()\n",
    "\n",
    "    def forward(self, output_student, output_teacher, temperature=1):\n",
    "        '''\n",
    "        Note: the output_student and output_teacher are logits \n",
    "        '''\n",
    "        T = temperature #.cuda()\n",
    "        \n",
    "        KD_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "            F.log_softmax(output_student/T, dim=-1),\n",
    "            F.softmax(output_teacher/T, dim=-1)\n",
    "        ) * T * T\n",
    "        \n",
    "        return KD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_div = DistillKL()\n",
    "criterion_cos = nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "lr = 5e-5\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "teacher_model = teacher_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 5\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metrics: ['f1', 'precision', 'recall']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load metrics suitable for multi-label classification\n",
    "metric_f1 = evaluate.load(\"f1\", config_name=\"multilabel\")\n",
    "metric_precision = evaluate.load(\"precision\", config_name=\"multilabel\")\n",
    "metric_recall = evaluate.load(\"recall\", config_name=\"multilabel\")\n",
    "\n",
    "# Optionally, define task_name if not already set\n",
    "task_name = \"go_emotions\"  # From your previous code, if applicable\n",
    "\n",
    "# You can still use a conditional if task_name is dynamic, but it's not tied to GLUE\n",
    "if task_name == \"go_emotions\":\n",
    "    # Use multi-label metrics\n",
    "    metrics = {\n",
    "        \"f1\": metric_f1,\n",
    "        \"precision\": metric_precision,\n",
    "        \"recall\": metric_recall\n",
    "    }\n",
    "else:\n",
    "    # Fallback for single-label tasks (if needed elsewhere)\n",
    "    metrics = {\"accuracy\": evaluate.load(\"accuracy\")}\n",
    "\n",
    "print(\"Loaded metrics:\", list(metrics.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create and distill students\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertConfig, AdamW, get_scheduler, DataCollatorWithPadding\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_id, num_labels=num_labels, id2label=id2label, label2id=label2id, problem_type=\"multi_label_classification\"\n",
    ")\n",
    "student_config = BertConfig.from_dict(teacher_model.config.to_dict())\n",
    "student_config.num_hidden_layers //= 2  # Reduce from 12 to 6\n",
    "student_config.problem_type = \"multi_label_classification\"  # Explicitly set for multi-label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_odd = AutoModelForSequenceClassification.from_config(student_config)\n",
    "distill_bert_weights(teacher=teacher_model, student=student_odd, layer_strategy='odd')  # Odd layers\n",
    "\n",
    "student_even = AutoModelForSequenceClassification.from_config(student_config)\n",
    "distill_bert_weights(teacher=teacher_model, student=student_even, layer_strategy='even')  # Even layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125404/.local/lib/python3.12/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  optimizer: Optimizer,\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model.to(device)\n",
    "student_odd.to(device)\n",
    "student_even.to(device)\n",
    "\n",
    "optimizer_odd = AdamW(student_odd.parameters(), lr=5e-5)\n",
    "optimizer_even = AdamW(student_even.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler_odd = get_scheduler(\"linear\", optimizer=optimizer_odd, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "lr_scheduler_even = get_scheduler(\"linear\", optimizer=optimizer_even, num_warmup_steps=0, num_training_steps=num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion_div = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "criterion_cos = torch.nn.CosineEmbeddingLoss()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"f1\": evaluate.load(\"f1\", config_name=\"multilabel\"),\n",
    "    \"precision\": evaluate.load(\"precision\", config_name=\"multilabel\"),\n",
    "    \"recall\": evaluate.load(\"recall\", config_name=\"multilabel\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 5000\n",
      "Evaluation batches: 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Evaluation batches: {len(eval_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8fb19d1f4f4a2dba248b65d6958a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Odd) - Avg train loss: 0.5129\n",
      "  - Loss_cls: 0.7404\n",
      "  - Loss_div: 0.0809\n",
      "  - Loss_cos: 0.7174\n",
      "Epoch 1 (Odd) - Eval loss: 0.7374, F1: 0.0861, Precision: 0.0459, Recall: 0.6826\n",
      "Epoch 2 (Odd) - Avg train loss: 0.5129\n",
      "  - Loss_cls: 0.7405\n",
      "  - Loss_div: 0.0809\n",
      "  - Loss_cos: 0.7173\n",
      "Epoch 2 (Odd) - Eval loss: 0.7374, F1: 0.0861, Precision: 0.0459, Recall: 0.6826\n",
      "Average Eval Metrics (Odd):\n",
      "  - F1: 0.0861\n",
      "  - Precision: 0.0459\n",
      "  - Recall: 0.6826\n",
      "Epoch 1 (Even) - Avg train loss: 0.4470\n",
      "  - Loss_cls: 0.7120\n",
      "  - Loss_div: 0.0498\n",
      "  - Loss_cos: 0.5791\n",
      "Epoch 1 (Even) - Eval loss: 0.7094, F1: 0.0869, Precision: 0.0467, Recall: 0.6349\n",
      "Epoch 2 (Even) - Avg train loss: 0.4475\n",
      "  - Loss_cls: 0.7121\n",
      "  - Loss_div: 0.0499\n",
      "  - Loss_cos: 0.5806\n",
      "Epoch 2 (Even) - Eval loss: 0.7094, F1: 0.0869, Precision: 0.0467, Recall: 0.6349\n",
      "Average Eval Metrics (Even):\n",
      "  - F1: 0.0869\n",
      "  - Precision: 0.0467\n",
      "  - Recall: 0.6349\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "student_odd = AutoModelForSequenceClassification.from_config(student_config)\n",
    "student_even = AutoModelForSequenceClassification.from_config(student_config)\n",
    "distill_bert_weights(teacher=teacher_model, student=student_odd, layer_strategy='odd')\n",
    "distill_bert_weights(teacher=teacher_model, student=student_even, layer_strategy='even')\n",
    "student_odd.to(device)\n",
    "student_even.to(device)\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_dataloader)  \n",
    "progress_bar = tqdm(range(num_training_steps * 2))  # For both odd and even\n",
    "\n",
    "# Training student_odd\n",
    "eval_metrics_odd = {\"f1\": 0, \"precision\": 0, \"recall\": 0}\n",
    "train_losses_odd = []\n",
    "train_losses_cls_odd = []\n",
    "train_losses_div_odd = []\n",
    "train_losses_cos_odd = []\n",
    "eval_losses_odd = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_odd.train()  # student_odd is the model object here\n",
    "    teacher_model.eval()\n",
    "    train_loss_odd = train_loss_cls_odd = train_loss_div_odd = train_loss_cos_odd = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        assert \"labels\" in batch, \"Missing 'labels' in batch\"  # Check labels in batch\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = student_odd(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_teacher = teacher_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            teacher_logits = output_teacher.logits\n",
    "\n",
    "        loss_cls = loss_fn(logits, batch[\"labels\"].float())\n",
    "        train_loss_cls_odd += loss_cls.item()\n",
    "        \n",
    "        student_log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        teacher_probs = torch.nn.functional.softmax(teacher_logits, dim=-1)\n",
    "        loss_div = criterion_div(student_log_probs, teacher_probs)\n",
    "        train_loss_div_odd += loss_div.item()\n",
    "\n",
    "        loss_cos = criterion_cos(teacher_logits, logits, torch.ones(logits.size(0)).to(device))\n",
    "        train_loss_cos_odd += loss_cos.item()\n",
    "        loss = (loss_cls + loss_div + loss_cos) / 3\n",
    "        train_loss_odd += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_odd.step()\n",
    "        lr_scheduler_odd.step()\n",
    "        optimizer_odd.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} (Odd) - Avg train loss: {train_loss_odd / len(train_dataloader):.4f}\")\n",
    "    print(f\"  - Loss_cls: {train_loss_cls_odd / len(train_dataloader):.4f}\")\n",
    "    print(f\"  - Loss_div: {train_loss_div_odd / len(train_dataloader):.4f}\")\n",
    "    print(f\"  - Loss_cos: {train_loss_cos_odd / len(train_dataloader):.4f}\")\n",
    "\n",
    "    train_losses_odd.append(train_loss_odd / len(train_dataloader))\n",
    "    train_losses_cls_odd.append(train_loss_cls_odd / len(train_dataloader))\n",
    "    train_losses_div_odd.append(train_loss_div_odd / len(train_dataloader))\n",
    "    train_losses_cos_odd.append(train_loss_cos_odd / len(train_dataloader))\n",
    "\n",
    "    # Eval metrics for student_odd\n",
    "    student_odd.eval()\n",
    "    eval_loss_odd = 0\n",
    "    all_predictions_odd = []\n",
    "    all_labels_odd = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            assert \"labels\" in batch, \"Missing 'labels' in eval batch\"  # Check eval batch\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = student_odd(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            logits = outputs.logits\n",
    "            loss_cls = loss_fn(logits, batch[\"labels\"].float())\n",
    "            eval_loss_odd += loss_cls.item()\n",
    "\n",
    "            predictions = (torch.sigmoid(logits) > 0.5).int()\n",
    "            all_predictions_odd.append(predictions.cpu().numpy())\n",
    "            all_labels_odd.append(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    all_predictions_odd = np.concatenate(all_predictions_odd, axis=0)\n",
    "    all_labels_odd = np.concatenate(all_labels_odd, axis=0)\n",
    "\n",
    "    eval_results_odd = {}\n",
    "    for metric_name, metric in metrics.items():\n",
    "        eval_results_odd[metric_name] = metric.compute(predictions=all_predictions_odd, references=all_labels_odd, average=\"micro\")[metric_name]\n",
    "        eval_metrics_odd[metric_name] += eval_results_odd[metric_name]\n",
    "\n",
    "    eval_losses_odd.append(eval_loss_odd / len(eval_dataloader))\n",
    "    print(f\"Epoch {epoch+1} (Odd) - Eval loss: {eval_losses_odd[-1]:.4f}, F1: {eval_results_odd['f1']:.4f}, Precision: {eval_results_odd['precision']:.4f}, Recall: {eval_results_odd['recall']:.4f}\")\n",
    "\n",
    "print(f\"Average Eval Metrics (Odd):\")\n",
    "for metric_name in eval_metrics_odd:\n",
    "    print(f\"  - {metric_name.capitalize()}: {eval_metrics_odd[metric_name] / num_epochs:.4f}\")\n",
    "\n",
    "# Training student_even\n",
    "eval_metrics_even = {\"f1\": 0, \"precision\": 0, \"recall\": 0}\n",
    "train_losses_even = []\n",
    "train_losses_cls_even = []\n",
    "train_losses_div_even = []\n",
    "train_losses_cos_even = []\n",
    "eval_losses_even = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_even.train()  # student_even is the model object here\n",
    "    teacher_model.eval()\n",
    "    train_loss_even = train_loss_cls_even = train_loss_div_even = train_loss_cos_even = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        assert \"labels\" in batch, \"Missing 'labels' in batch\"  # Check labels in batch\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = student_even(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_teacher = teacher_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            teacher_logits = output_teacher.logits\n",
    "\n",
    "        loss_cls = loss_fn(logits, batch[\"labels\"].float())\n",
    "        train_loss_cls_even += loss_cls.item()\n",
    "        \n",
    "        student_log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        teacher_probs = torch.nn.functional.softmax(teacher_logits, dim=-1)\n",
    "        loss_div = criterion_div(student_log_probs, teacher_probs)\n",
    "        train_loss_div_even += loss_div.item()\n",
    "\n",
    "        loss_cos = criterion_cos(teacher_logits, logits, torch.ones(logits.size(0)).to(device))\n",
    "        train_loss_cos_even += loss_cos.item()\n",
    "        loss = (loss_cls + loss_div + loss_cos) / 3\n",
    "        train_loss_even += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_even.step()\n",
    "        lr_scheduler_even.step()\n",
    "        optimizer_even.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} (Even) - Avg train loss: {train_loss_even / len(train_dataloader):.4f}\")\n",
    "    print(f\"  - Loss_cls: {train_loss_cls_even / len(train_dataloader):.4f}\")\n",
    "    print(f\"  - Loss_div: {train_loss_div_even / len(train_dataloader):.4f}\")\n",
    "    print(f\"  - Loss_cos: {train_loss_cos_even / len(train_dataloader):.4f}\")\n",
    "\n",
    "    train_losses_even.append(train_loss_even / len(train_dataloader))\n",
    "    train_losses_cls_even.append(train_loss_cls_even / len(train_dataloader))\n",
    "    train_losses_div_even.append(train_loss_div_even / len(train_dataloader))\n",
    "    train_losses_cos_even.append(train_loss_cos_even / len(train_dataloader))\n",
    "\n",
    "    # Eval metrics for student_even\n",
    "    student_even.eval()\n",
    "    eval_loss_even = 0\n",
    "    all_predictions_even = []\n",
    "    all_labels_even = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            assert \"labels\" in batch, \"Missing 'labels' in eval batch\"  # Check eval batch\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = student_even(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            logits = outputs.logits\n",
    "            loss_cls = loss_fn(logits, batch[\"labels\"].float())\n",
    "            eval_loss_even += loss_cls.item()\n",
    "\n",
    "            predictions = (torch.sigmoid(logits) > 0.5).int()\n",
    "            all_predictions_even.append(predictions.cpu().numpy())\n",
    "            all_labels_even.append(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    all_predictions_even = np.concatenate(all_predictions_even, axis=0)\n",
    "    all_labels_even = np.concatenate(all_labels_even, axis=0)\n",
    "\n",
    "    eval_results_even = {}\n",
    "    for metric_name, metric in metrics.items():\n",
    "        eval_results_even[metric_name] = metric.compute(predictions=all_predictions_even, references=all_labels_even, average=\"micro\")[metric_name]\n",
    "        eval_metrics_even[metric_name] += eval_results_even[metric_name]\n",
    "\n",
    "    eval_losses_even.append(eval_loss_even / len(eval_dataloader))\n",
    "    print(f\"Epoch {epoch+1} (Even) - Eval loss: {eval_losses_even[-1]:.4f}, F1: {eval_results_even['f1']:.4f}, Precision: {eval_results_even['precision']:.4f}, Recall: {eval_results_even['recall']:.4f}\")\n",
    "\n",
    "print(f\"Average Eval Metrics (Even):\")\n",
    "for metric_name in eval_metrics_even:\n",
    "    print(f\"  - {metric_name.capitalize()}: {eval_metrics_even[metric_name] / num_epochs:.4f}\")\n",
    "\n",
    "student_odd.save_pretrained(\"./student_odd_model1\")\n",
    "student_even.save_pretrained(\"./student_even_model1\")\n",
    "tokenizer.save_pretrained(\"./student_odd_model1\")\n",
    "tokenizer.save_pretrained(\"./student_even_model1\")\n",
    "print(\"Models saved: ./student_odd_model1 and ./student_even_model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.82956489920616\n"
     ]
    }
   ],
   "source": [
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and tokenizer saved as .pkl files: student_odd_model.pkl, student_even_model.pkl, tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the full models and tokenizer as .pkl files\n",
    "with open(\"./student_odd_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(student_odd, f)\n",
    "with open(\"./student_even_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(student_even, f)\n",
    "with open(\"./tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Models and tokenizer saved as .pkl files: student_odd_model.pkl, student_even_model.pkl, tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the losses for student_odd and student_even\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Training Losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses_odd, label='Train Loss (Odd)', color='blue')\n",
    "plt.plot(train_losses_even, label='Train Loss (Even)', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Evaluation Losses\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_losses_odd, label='Eval Loss (Odd)', color='blue')\n",
    "plt.plot(eval_losses_even, label='Eval Loss (Even)', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evaluation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and tokenizers saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Save the model with pickle\n",
    "with open('./student_odd_model_updated.pkl', 'wb') as f:\n",
    "    pickle.dump(student_odd, f)\n",
    "\n",
    "with open('./student_even_model_updated.pkl', 'wb') as f:\n",
    "    pickle.dump(student_even, f)\n",
    "\n",
    "# Save the tokenizers\n",
    "tokenizer.save_pretrained('./student_odd_model2')  # Save the tokenizer for student_odd\n",
    "tokenizer.save_pretrained('./student_even_model2')  # Save the tokenizer for student_even\n",
    "\n",
    "# Save the configuration of the models\n",
    "student_odd.config.save_pretrained('./student_odd_model2')  # Save the config of student_odd\n",
    "student_even.config.save_pretrained('./student_even_model2')  # Save the config of student_even\n",
    "\n",
    "print(\"Models and tokenizers saved.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix (Teacher Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1f40c9fdea44a4aa35dc557f3bba62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss 0.1070\n",
      "Epoch 1: Eval loss 0.0880, Eval Acc 0.4000\n",
      "Epoch 2: Train loss 0.0825\n",
      "Epoch 2: Eval loss 0.0848, Eval Acc 0.4480\n",
      "Epoch 3: Train loss 0.0690\n",
      "Epoch 3: Eval loss 0.0893, Eval Acc 0.4460\n",
      "Avg Eval Accuracy: 0.4313\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Ensure teacher_model is moved to the correct device\n",
    "teacher_model.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 5e-5\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "# Optimizer (no scheduler in your original code; I removed lr_scheduler)\n",
    "optimizer = optim.Adam(params=teacher_model.parameters(), lr=lr)\n",
    "\n",
    "# Loss function for multi-label classification\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# To track training and evaluation metrics\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "eval_metrics = {\"accuracy\": 0}\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    teacher_model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass (do not pass labels to compute internal loss)\n",
    "        output_teacher = teacher_model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            token_type_ids=batch[\"token_type_ids\"],\n",
    "            labels=None  # Set to None to get logits only\n",
    "        )\n",
    "        \n",
    "        # Compute multi-label loss\n",
    "        logits = output_teacher.logits  # Shape: [batch_size, num_labels]\n",
    "        labels = batch[\"labels\"].float()  # Convert to float for BCEWithLogitsLoss\n",
    "        loss = loss_fn(logits, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f'Epoch {epoch+1}: Train loss {avg_train_loss:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    teacher_model.eval()\n",
    "    eval_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = teacher_model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                token_type_ids=batch[\"token_type_ids\"],\n",
    "                labels=None\n",
    "            )\n",
    "        \n",
    "        # Compute evaluation loss\n",
    "        logits = outputs.logits\n",
    "        labels = batch[\"labels\"].float()\n",
    "        eval_loss += loss_fn(logits, labels).item()\n",
    "\n",
    "        # Get predictions (apply sigmoid and threshold at 0.5)\n",
    "        predictions = (torch.sigmoid(logits) > 0.5).int()  # Shape: [batch_size, num_labels]\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_losses.append(avg_eval_loss)\n",
    "\n",
    "    # Flatten lists for accuracy computation\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)  # Shape: [total_samples, num_labels]\n",
    "    all_labels = np.concatenate(all_labels, axis=0)  # Shape: [total_samples, num_labels]\n",
    "\n",
    "    # Compute evaluation accuracy (exact match ratio for multi-label)\n",
    "    eval_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    eval_metrics[\"accuracy\"] += eval_accuracy\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Eval loss {avg_eval_loss:.4f}, Eval Acc {eval_accuracy:.4f}\")\n",
    "\n",
    "# Average metrics over all epochs\n",
    "avg_eval_accuracy = eval_metrics[\"accuracy\"] / num_epochs\n",
    "print(f'Avg Eval Accuracy: {avg_eval_accuracy:.4f}')\n",
    "\n",
    "# Optionally save the model after training\n",
    "# teacher_model.save_pretrained('./teacher_model')\n",
    "# tokenizer.save_pretrained('./teacher_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing test loss for student_odd...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37d92dca3dc4485b640c59c524788d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Odd Test Loss: 0.7386\n",
      "Computing test loss for student_even...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7347e0fe2a449e95c636ff20ac0db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Even Test Loss: 0.7096\n",
      "Computing test loss for teacher_model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5469a946a3467a9751785645c98544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Model Test Loss: 0.0884\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Ensure models are on the correct device\n",
    "student_odd.to(device)\n",
    "student_even.to(device)\n",
    "teacher_model.to(device)  # Optional, for comparison\n",
    "\n",
    "# Loss function for multi-label classification\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Use the existing test_dataloader from your notebook (cell 19)\n",
    "# test_dataloader = DataLoader(small_test_dataset, batch_size=32, collate_fn=data_collator)\n",
    "# Already defined, so no need to redefine unless you want to change batch_size\n",
    "\n",
    "# Function to compute test loss for a given model\n",
    "def compute_test_loss(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating Test Set\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                token_type_ids=batch[\"token_type_ids\"],\n",
    "                labels=None  # Get logits only\n",
    "            )\n",
    "            logits = outputs.logits  # Shape: [batch_size, num_labels]\n",
    "            labels = batch[\"labels\"].float()  # Convert to float for BCEWithLogitsLoss\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_test_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_test_loss = total_test_loss / num_batches\n",
    "    return avg_test_loss\n",
    "\n",
    "# Compute test loss for both student models\n",
    "print(\"Computing test loss for student_odd...\")\n",
    "test_loss_odd = compute_test_loss(student_odd, test_dataloader, loss_fn, device)\n",
    "print(f\"Student Odd Test Loss: {test_loss_odd:.4f}\")\n",
    "\n",
    "print(\"Computing test loss for student_even...\")\n",
    "test_loss_even = compute_test_loss(student_even, test_dataloader, loss_fn, device)\n",
    "print(f\"Student Even Test Loss: {test_loss_even:.4f}\")\n",
    "\n",
    "# Optionally compute test loss for teacher model for comparison\n",
    "print(\"Computing test loss for teacher_model...\")\n",
    "test_loss_teacher = compute_test_loss(teacher_model, test_dataloader, loss_fn, device)\n",
    "print(f\"Teacher Model Test Loss: {test_loss_teacher:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text: 'Iâ€™m so happy to see you all again!'\n",
      "Teacher Model:\n",
      "  Predicted Emotions: ['joy']\n",
      "  Probabilities: ['joy: 0.884']\n",
      "Student Odd:\n",
      "  Predicted Emotions: ['amusement', 'confusion', 'curiosity', 'desire', 'disapproval', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'pride', 'realization', 'relief', 'remorse', 'surprise', 'neutral']\n",
      "  Probabilities: ['admiration: 0.303', 'amusement: 0.601', 'anger: 0.469', 'annoyance: 0.480', 'approval: 0.378', 'caring: 0.414', 'confusion: 0.502', 'curiosity: 0.590', 'desire: 0.614', 'disappointment: 0.411', 'disapproval: 0.546', 'disgust: 0.298', 'embarrassment: 0.528', 'excitement: 0.549', 'fear: 0.541', 'gratitude: 0.605', 'grief: 0.567', 'joy: 0.633', 'love: 0.635', 'nervousness: 0.535', 'optimism: 0.427', 'pride: 0.582', 'realization: 0.614', 'relief: 0.566', 'remorse: 0.552', 'sadness: 0.424', 'surprise: 0.500', 'neutral: 0.614']\n",
      "Student Even:\n",
      "  Predicted Emotions: ['amusement', 'curiosity', 'desire', 'disapproval', 'embarrassment', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'pride', 'realization', 'relief', 'remorse', 'surprise', 'neutral']\n",
      "  Probabilities: ['admiration: 0.368', 'amusement: 0.549', 'anger: 0.496', 'annoyance: 0.444', 'approval: 0.397', 'caring: 0.461', 'confusion: 0.425', 'curiosity: 0.578', 'desire: 0.569', 'disappointment: 0.441', 'disapproval: 0.523', 'disgust: 0.357', 'embarrassment: 0.546', 'excitement: 0.493', 'fear: 0.483', 'gratitude: 0.560', 'grief: 0.540', 'joy: 0.535', 'love: 0.604', 'nervousness: 0.551', 'optimism: 0.442', 'pride: 0.570', 'realization: 0.602', 'relief: 0.556', 'remorse: 0.551', 'sadness: 0.468', 'surprise: 0.503', 'neutral: 0.562']\n",
      "\n",
      "Input Text: 'This is absolutely disgusting and frustrating.'\n",
      "Teacher Model:\n",
      "  Predicted Emotions: ['disgust']\n",
      "  Probabilities: ['annoyance: 0.159', 'disgust: 0.875']\n",
      "Student Odd:\n",
      "  Predicted Emotions: ['amusement', 'anger', 'annoyance', 'confusion', 'curiosity', 'desire', 'embarrassment', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'pride', 'realization', 'relief', 'remorse', 'neutral']\n",
      "  Probabilities: ['admiration: 0.330', 'amusement: 0.634', 'anger: 0.562', 'annoyance: 0.503', 'approval: 0.484', 'caring: 0.475', 'confusion: 0.548', 'curiosity: 0.573', 'desire: 0.567', 'disappointment: 0.433', 'disapproval: 0.467', 'disgust: 0.358', 'embarrassment: 0.503', 'excitement: 0.499', 'fear: 0.524', 'gratitude: 0.596', 'grief: 0.599', 'joy: 0.639', 'love: 0.637', 'nervousness: 0.604', 'optimism: 0.463', 'pride: 0.501', 'realization: 0.537', 'relief: 0.570', 'remorse: 0.600', 'sadness: 0.349', 'surprise: 0.461', 'neutral: 0.611']\n",
      "Student Even:\n",
      "  Predicted Emotions: ['amusement', 'curiosity', 'desire', 'disapproval', 'embarrassment', 'excitement', 'gratitude', 'grief', 'love', 'nervousness', 'pride', 'realization', 'relief', 'remorse', 'neutral']\n",
      "  Probabilities: ['admiration: 0.387', 'amusement: 0.536', 'anger: 0.495', 'annoyance: 0.450', 'approval: 0.413', 'caring: 0.465', 'confusion: 0.457', 'curiosity: 0.562', 'desire: 0.556', 'disappointment: 0.447', 'disapproval: 0.524', 'disgust: 0.340', 'embarrassment: 0.526', 'excitement: 0.517', 'fear: 0.488', 'gratitude: 0.542', 'grief: 0.515', 'joy: 0.488', 'love: 0.608', 'nervousness: 0.535', 'optimism: 0.425', 'pride: 0.569', 'realization: 0.576', 'relief: 0.573', 'remorse: 0.595', 'sadness: 0.475', 'surprise: 0.497', 'neutral: 0.555']\n",
      "\n",
      "Input Text: 'Wow, I didnâ€™t expect that at all!'\n",
      "Teacher Model:\n",
      "  Predicted Emotions: ['surprise']\n",
      "  Probabilities: ['excitement: 0.133', 'surprise: 0.834']\n",
      "Student Odd:\n",
      "  Predicted Emotions: ['amusement', 'anger', 'confusion', 'curiosity', 'desire', 'disapproval', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'pride', 'realization', 'relief', 'remorse', 'neutral']\n",
      "  Probabilities: ['admiration: 0.317', 'amusement: 0.654', 'anger: 0.501', 'annoyance: 0.455', 'approval: 0.434', 'caring: 0.461', 'confusion: 0.525', 'curiosity: 0.574', 'desire: 0.575', 'disappointment: 0.438', 'disapproval: 0.509', 'disgust: 0.374', 'embarrassment: 0.495', 'excitement: 0.524', 'fear: 0.518', 'gratitude: 0.600', 'grief: 0.615', 'joy: 0.611', 'love: 0.601', 'nervousness: 0.581', 'optimism: 0.475', 'pride: 0.529', 'realization: 0.556', 'relief: 0.547', 'remorse: 0.533', 'sadness: 0.366', 'surprise: 0.478', 'neutral: 0.587']\n",
      "Student Even:\n",
      "  Predicted Emotions: ['amusement', 'curiosity', 'desire', 'disapproval', 'embarrassment', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'pride', 'realization', 'relief', 'remorse', 'surprise', 'neutral']\n",
      "  Probabilities: ['admiration: 0.377', 'amusement: 0.541', 'anger: 0.496', 'annoyance: 0.440', 'approval: 0.398', 'caring: 0.465', 'confusion: 0.422', 'curiosity: 0.567', 'desire: 0.570', 'disappointment: 0.437', 'disapproval: 0.524', 'disgust: 0.360', 'embarrassment: 0.544', 'excitement: 0.490', 'fear: 0.482', 'gratitude: 0.561', 'grief: 0.532', 'joy: 0.531', 'love: 0.597', 'nervousness: 0.551', 'optimism: 0.446', 'pride: 0.566', 'realization: 0.598', 'relief: 0.559', 'remorse: 0.541', 'sadness: 0.470', 'surprise: 0.508', 'neutral: 0.558']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Ensure models are on the correct device\n",
    "teacher_model.to(device)\n",
    "student_odd.to(device)\n",
    "student_even.to(device)\n",
    "\n",
    "\n",
    "# Function for inference\n",
    "def predict_emotions(text, model, tokenizer, id2label, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the correct device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # Shape: [1, num_labels]\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.sigmoid(logits).squeeze(0)  # Shape: [num_labels]\n",
    "    \n",
    "    # Apply threshold to get predicted labels\n",
    "    predicted_labels = (probs > threshold).int()\n",
    "    \n",
    "    # Get the emotion names\n",
    "    emotions = [id2label[i] for i, val in enumerate(predicted_labels) if val == 1]\n",
    "    \n",
    "    # Return probabilities and emotions\n",
    "    return probs.cpu().numpy(), emotions\n",
    "\n",
    "# Example text for inference\n",
    "example_texts = [\n",
    "    \"Iâ€™m so happy to see you all again!\",\n",
    "    \"This is absolutely disgusting and frustrating.\",\n",
    "    \"Wow, I didnâ€™t expect that at all!\"\n",
    "]\n",
    "\n",
    "# Perform inference with all models\n",
    "models = {\n",
    "    \"Teacher Model\": teacher_model,\n",
    "    \"Student Odd\": student_odd,\n",
    "    \"Student Even\": student_even\n",
    "}\n",
    "\n",
    "for text in example_texts:\n",
    "    print(f\"\\nInput Text: '{text}'\")\n",
    "    for model_name, model in models.items():\n",
    "        probs, emotions = predict_emotions(text, model, tokenizer, id2label, device)\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  Predicted Emotions: {emotions if emotions else 'None'}\")\n",
    "        print(f\"  Probabilities: {[f'{id2label[i]}: {prob:.3f}' for i, prob in enumerate(probs) if prob > 0.1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
