{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/quicktour)\n",
    "\n",
    "PEFT methods selectively adjust a small set of additional model parameters while keeping the majority of the pre-trained LLM's parameters unchanged. This significantly reduces computational and storage requirements and addresses the problem of catastrophic forgetting often seen during full fine-tuning. Additionally, PEFT methods outperform traditional fine-tuning in situations with limited data and demonstrate superior generalization to out-of-domain scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.15.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (2.2.2+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (4.49.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (1.4.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (0.29.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2024.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.15.0-py3-none-any.whl (410 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install peft\n",
    "# !pip install evaluate\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.49.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import peft\n",
    "peft.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "\n",
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Techniques     | All Params  | Trainable Params | Trainable % |\n",
    "|----------------|-------------|------------------|-------------|\n",
    "| BitFit         | 124,808,448 | 102,144          | 0.082       |\n",
    "| Adapter        | 124,808,448 | 894,528          | 0.714       |\n",
    "| Prompt Tuning  | 124,808,448 | 6,144            | 0.004       |\n",
    "| Prefix  Tuning | 124,808,448 | 368,640          | 0.295       |\n",
    "| LoRA           | 124,808,448 | 294,912          | 0.236       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_nb_trainable_parameters():\n",
    "    r\"\"\"\n",
    "    Returns the number of trainable parameters and number of all parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        # Due to the design of 4bit linear layers from bitsandbytes\n",
    "        # one needs to multiply the number of parameters by 2 to get\n",
    "        # the correct number of parameters\n",
    "        if param.__class__.__name__ == \"Params4bit\":\n",
    "            num_params = num_params * 2\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    return trainable_params, all_param\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params, all_param = get_nb_trainable_parameters()\n",
    "\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BitFit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../figures/bitfit.pbm\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, GPT2TokenizerFast\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer_name_or_path = \"gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters except biases\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 102144 || all params: 124439808 || trainable%: 0.08208305818022477\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter\n",
    "\n",
    "You can read more [Adapter paper](https://arxiv.org/pdf/1902.00751)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../figures/adapter.webp\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2AdapterModel(\n",
       "  (shared_parameters): ModuleDict()\n",
       "  (transformer): GPT2Model(\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): MergedLinear(\n",
       "            in_features=768, out_features=2304, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (prefix_tuning): PrefixTuningShim(\n",
       "            (prefix_gates): ModuleDict()\n",
       "            (pool): PrefixTuningPool(\n",
       "              (prefix_tunings): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (c_proj): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict(\n",
       "            (adapter-name): Adapter(\n",
       "              (non_linearity): Activation_Function_Class(\n",
       "                (f): ReLU()\n",
       "              )\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (lm_head): CausalLMHead(\n",
       "      (0): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caution!!!! Adapter-transformers have depreacted and conflicted \n",
    "# adapter-transformers required transformers version 4.28.1 which is quite old already\n",
    "# there is no need to use anymore\n",
    "# !pip install adapter-transformers\n",
    "from transformers.adapters import GPT2AdapterModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "model = GPT2AdapterModel(config)\n",
    "model.freeze_model()\n",
    "model.add_causal_lm_head(\n",
    "    head_name = 'lm_head',\n",
    ")\n",
    "model.add_adapter(adapter_name = 'adapter-name')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 102144 || all params: 124439808 || trainable%: 0.08208305818022477\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Tuning\n",
    "[Soft prompt tuning](https://arxiv.org/pdf/2104.08691) (Lester et al. 2021) concatenates the embeddings of the input tokens with a trainable tensor that can be optimized via backpropagation to improve the modeling performance on a target task. These soft prompts are continuous vectors in the model's embedding space that get optimized during training. Unlike traditional discrete prompts that use natural language tokens, soft prompts have no inherent meaning but learn to elicit the desired behavior from the frozen model through gradient descent. The technique is particularly effective for multi-task scenarios since each task requires storing only a small prompt vector (typically a few hundred parameters) rather than a full model copy. This approach not only maintains a minimal memory footprint but also enables rapid task switching by simply swapping prompt vectors without any model reloading.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../figures/prompt.webp\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,360 || all params: 124,455,168 || trainable%: 0.0123\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import (\n",
    "    get_peft_model, \n",
    "    PromptTuningInit, \n",
    "    PromptTuningConfig, \n",
    "    TaskType, \n",
    "    )\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer_name_or_path = \"gpt2\"\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    peft_type=\"PROMPT_TUNING\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=20,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    ")\n",
    "\n",
    "# Create prompt-tunable model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix Tuning\n",
    "To add trainable tensors to each transformer block instead of only the input embeddings, as in soft prompt tuning. Also, we obtain the soft prompt embedding via fully connected layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../figures/prefix_adapterhub.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 368,640 || all params: 124,808,448 || trainable%: 0.2954\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoModelForCausalLM\n",
    "from peft import get_peft_model, PrefixTuningConfig, TaskType\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer_name_or_path = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    num_virtual_tokens=20\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA works by adding pairs of rank decomposition matrices to transformer layers, typically focusing on attention weights. During inference, these adapter weights can be merged with the base model, resulting in no additional latency overhead. LoRA is particularly useful for adapting large language models to specific tasks or domains while keeping resource requirements manageable. You can read more about LoRA in the [LoRA paper](https://arxiv.org/pdf/2106.09685).\n",
    "\n",
    "LoRA works by adding pairs of rank decomposition matrices to transformer layers, typically focusing on attention weights. During inference, these adapter weights can be merged with the base model, resulting in no additional latency overhead. LoRA is particularly useful for adapting large language models to specific tasks or domains while keeping resource requirements manageable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../figures/lora-2.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer_name_or_path = \"gpt2\"\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Create prompt-tunable model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer_name_or_path = \"gpt2\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with LoRA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[explore more](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Other Methods\n",
    "\n",
    "When compared to other PEFT approaches, prompt tuning stands out for its efficiency. While LoRA offers low parameter counts and memory usage but requires loading adapters for task switching, prompt tuning achieves even lower resource usage and enables immediate task switching. Full fine-tuning, in contrast, demands significant resources and requires separate model copies for different tasks.\n",
    "\n",
    "| Method | Parameters | Memory | Task Switching |\n",
    "|--------|------------|---------|----------------|\n",
    "| Prompt Tuning | Very Low | Minimal | Easy |\n",
    "| LoRA | Low | Low | Requires Loading |\n",
    "| Full Fine-tuning | High | High | New Model Copy |\n",
    "\n",
    "When implementing prompt tuning, start with a small number of virtual tokens (8-16) and increase only if the task complexity demands it. Text initialization typically yields better results than random initialization, especially when using task-relevant text. The initialization strategy should reflect the complexity of your target task.\n",
    "\n",
    "Training requires slightly different considerations than full fine-tuning. Higher learning rates often work well, but careful monitoring of prompt token gradients is essential. Regular validation on diverse examples helps ensure robust performance across different scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the simplified GoEmotions dataset\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    # Convert labels to a 2D tensor (batch size Ã— number of labels)\n",
    "    tokenized[\"labels\"] = [[float(label) for label in labels] for labels in examples[\"labels\"]]\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/43410 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43410/43410 [00:02<00:00, 21281.47 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5426/5426 [00:00<00:00, 21111.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5427/5427 [00:00<00:00, 20394.47 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', 'id', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'id', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"My favourite food is anything I didn't have to cook myself.\",\n",
       " 'labels': [27],\n",
       " 'id': 'eebbqej',\n",
       " 'input_ids': [3666,\n",
       "  12507,\n",
       "  2057,\n",
       "  318,\n",
       "  1997,\n",
       "  314,\n",
       "  1422,\n",
       "  470,\n",
       "  423,\n",
       "  284,\n",
       "  4255,\n",
       "  3589,\n",
       "  13,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and eval sets\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2ForSequenceClassification\n",
    "\n",
    "def initialize_student_model(teacher_model, layers_to_copy):\n",
    "    \"\"\"\n",
    "    Initialize a 6-layer student model using specific layers from the 12-layer teacher model.\n",
    "    \n",
    "    Args:\n",
    "        teacher_model: The 12-layer GPT-2 teacher model.\n",
    "        layers_to_copy: List of layer indices to copy from the teacher model (e.g., [1, 3, 5, 7, 9, 11] for odd layers).\n",
    "    \n",
    "    Returns:\n",
    "        student_model: The 6-layer student model initialized with the specified layers.\n",
    "    \"\"\"\n",
    "    # Define a 6-layer GPT-2 configuration\n",
    "    config = GPT2Config.from_pretrained(\"gpt2\", num_hidden_layers=6, num_labels=28)  # 28 emotions in GoEmotions\n",
    "    \n",
    "    # Load the 6-layer GPT-2 model\n",
    "    student_model = GPT2ForSequenceClassification(config)\n",
    "    \n",
    "    # Copy weights from the teacher model to the student model\n",
    "    for i, layer_idx in enumerate(layers_to_copy):\n",
    "        # Copy transformer layer weights\n",
    "        student_model.transformer.h[i].load_state_dict(teacher_model.transformer.h[layer_idx].state_dict())\n",
    "    \n",
    "    # Copy embedding and positional embedding weights\n",
    "    student_model.transformer.wte.load_state_dict(teacher_model.transformer.wte.state_dict())\n",
    "    student_model.transformer.wpe.load_state_dict(teacher_model.transformer.wpe.state_dict())\n",
    "    \n",
    "    # Copy the classification head weights (if applicable)\n",
    "    if hasattr(teacher_model, \"classifier\"):\n",
    "        student_model.classifier.load_state_dict(teacher_model.classifier.state_dict())\n",
    "    \n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "class GPT2ForMultiLabelSequenceClassification(GPT2ForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Replace the default classification head with a multi-label head\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get the outputs from the base GPT-2 model\n",
    "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get the hidden states of the last layer\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Get the pooled output (CLS token for classification)\n",
    "        pooled_output = hidden_states[:, 0, :]\n",
    "        \n",
    "        # Pass through the classification head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        # Apply sigmoid for multi-label classification\n",
    "        logits = self.sigmoid(logits)\n",
    "        \n",
    "        # Compute the loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCELoss()  # Binary Cross-Entropy Loss for multi-label classification\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 6-layer GPT-2 configuration\n",
    "config = GPT2Config.from_pretrained(\"gpt2\", num_hidden_layers=6, num_labels=28)  # 28 emotions in GoEmotions\n",
    "\n",
    "# Load the 6-layer GPT-2 model with the custom multi-label head\n",
    "student_model_odd = GPT2ForMultiLabelSequenceClassification(config)\n",
    "student_model_even = GPT2ForMultiLabelSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 190,492 || all params: 82,146,104 || trainable%: 0.2319\n",
      "trainable params: 190,492 || all params: 82,146,104 || trainable%: 0.2319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    inference_mode=False,\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    bias=\"none\",  # No bias\n",
    ")\n",
    "\n",
    "# Apply LoRA to the Odd Layer student model\n",
    "student_model_odd = get_peft_model(student_model_odd, peft_config)\n",
    "student_model_odd.print_trainable_parameters()\n",
    "\n",
    "# Apply LoRA to the Even Layer student model\n",
    "student_model_even = get_peft_model(student_model_even, peft_config)\n",
    "student_model_even.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-lora-6layer-goemotions-simplified\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2ForSequenceClassification\n",
    "\n",
    "# Define a 6-layer GPT-2 configuration\n",
    "config = GPT2Config.from_pretrained(\"gpt2\", num_hidden_layers=6, num_labels=28)  # 28 emotions in GoEmotions\n",
    "\n",
    "# Load the 6-layer GPT-2 model\n",
    "student_model = GPT2ForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 168,960 || all params: 82,103,040 || trainable%: 0.2058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    inference_mode=False,\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    bias=\"none\",  # No bias\n",
    ")\n",
    "\n",
    "# Apply LoRA to the 6-layer student model\n",
    "student_model = get_peft_model(student_model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "student_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-lora-6layer-goemotions-simplified\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_20864\\4277373701.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GoEmotions dataset\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "num_labels = 28  # GoEmotions has 27 emotions + neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process multi-label to single-label\n",
    "def process_labels(example):\n",
    "    # Take the first label as the primary emotion\n",
    "    example['labels'] = example['labels'][0] if example['labels'] else 0  # Default to 0 (neutral) if empty\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 6-layer student model configurations\n",
    "def create_student_model(initialization_type=\"lora\"):\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=num_labels,\n",
    "        dropout=0.1,\n",
    "        attention_dropout=0.1\n",
    "    )\n",
    "    \n",
    "    num_layers = len(base_model.distilbert.transformer.layer)  # Should be 6\n",
    "    \n",
    "    if initialization_type == \"lora\":\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            inference_mode=False,\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_lin\", \"v_lin\"]\n",
    "        )\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "    \n",
    "    elif initialization_type == \"odd\":\n",
    "        pretrained = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            dropout=0.1,\n",
    "            attention_dropout=0.1\n",
    "        )\n",
    "        odd_mapping = [1, 3, 5]\n",
    "        for i, target_idx in enumerate(range(0, num_layers, 2)):\n",
    "            if i < len(odd_mapping):\n",
    "                base_model.distilbert.transformer.layer[target_idx] = pretrained.distilbert.transformer.layer[odd_mapping[i]]\n",
    "        model = base_model\n",
    "    \n",
    "    elif initialization_type == \"even\":\n",
    "        pretrained = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            dropout=0.1,\n",
    "            attention_dropout=0.1\n",
    "        )\n",
    "        even_mapping = [0, 2, 4]\n",
    "        for i, target_idx in enumerate(range(0, num_layers, 2)):\n",
    "            if i < len(even_mapping):\n",
    "                base_model.distilbert.transformer.layer[target_idx] = pretrained.distilbert.transformer.layer[even_mapping[i]]\n",
    "        model = base_model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5426/5426 [00:00<00:00, 8032.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Prepare dataset\n",
    "# First process labels, then tokenize\n",
    "tokenized_datasets = dataset.map(process_labels)\n",
    "tokenized_datasets = tokenized_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['text', 'id'])\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43410/43410 [00:01<00:00, 38231.09 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5426/5426 [00:00<00:00, 30473.60 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5427/5427 [00:00<00:00, 31927.28 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43410/43410 [00:01<00:00, 28352.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5426/5426 [00:00<00:00, 24453.64 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5427/5427 [00:00<00:00, 26187.17 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_9852\\729855277.py:108: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LoRA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:402: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8142' max='8142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8142/8142 07:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.858500</td>\n",
       "      <td>1.749838</td>\n",
       "      <td>0.488299</td>\n",
       "      <td>0.413590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.641800</td>\n",
       "      <td>1.606364</td>\n",
       "      <td>0.521835</td>\n",
       "      <td>0.476836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.559500</td>\n",
       "      <td>1.578487</td>\n",
       "      <td>0.529390</td>\n",
       "      <td>0.486098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA - Trainable params: 759580 || All params: 67734584\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA - Evaluation results: {'eval_loss': 1.5784870386123657, 'eval_accuracy': 0.529390086604017, 'eval_f1': 0.4860976967543385, 'eval_runtime': 8.7276, 'eval_samples_per_second': 621.824, 'eval_steps_per_second': 38.957, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_9852\\729855277.py:108: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Odd Layer model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8142' max='8142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8142/8142 16:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.600100</td>\n",
       "      <td>1.519223</td>\n",
       "      <td>0.551502</td>\n",
       "      <td>0.524302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.346300</td>\n",
       "      <td>1.440848</td>\n",
       "      <td>0.569560</td>\n",
       "      <td>0.557456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.160500</td>\n",
       "      <td>1.438388</td>\n",
       "      <td>0.571955</td>\n",
       "      <td>0.557387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd Layer - Trainable params: 66975004 || All params: 66975004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd Layer - Evaluation results: {'eval_loss': 1.4383876323699951, 'eval_accuracy': 0.5719550396167311, 'eval_f1': 0.557387104127604, 'eval_runtime': 8.6893, 'eval_samples_per_second': 624.559, 'eval_steps_per_second': 39.128, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_9852\\729855277.py:108: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Even Layer model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8142' max='8142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8142/8142 16:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.503700</td>\n",
       "      <td>1.441789</td>\n",
       "      <td>0.571402</td>\n",
       "      <td>0.548567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.256300</td>\n",
       "      <td>1.389252</td>\n",
       "      <td>0.577667</td>\n",
       "      <td>0.567097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.067200</td>\n",
       "      <td>1.400710</td>\n",
       "      <td>0.582458</td>\n",
       "      <td>0.570338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Layer - Trainable params: 66975004 || All params: 66975004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Layer - Evaluation results: {'eval_loss': 1.3892524242401123, 'eval_accuracy': 0.5776672194582643, 'eval_f1': 0.5670966472203001, 'eval_runtime': 9.0121, 'eval_samples_per_second': 602.193, 'eval_steps_per_second': 37.727, 'epoch': 3.0}\n",
      "\n",
      "Performance Comparison:\n",
      "| Model | Accuracy | F1 Score | Trainable Parameters |\n",
      "|-------|----------|----------|---------------------|\n",
      "| LoRA | 0.5294 | 0.4861 | 759580 |\n",
      "| Odd Layer | 0.5720 | 0.5574 | 66975004 |\n",
      "| Even Layer | 0.5777 | 0.5671 | 66975004 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./best_lora_model\\\\tokenizer_config.json',\n",
       " './best_lora_model\\\\special_tokens_map.json',\n",
       " './best_lora_model\\\\vocab.txt',\n",
       " './best_lora_model\\\\added_tokens.json',\n",
       " './best_lora_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(model, name):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name} - Trainable params: {trainable_params} || All params: {total_params}\")\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"{name} - Evaluation results: {eval_results}\")\n",
    "    return eval_results\n",
    "\n",
    "# Train and compare all three configurations\n",
    "models = {\n",
    "    \"LoRA\": create_student_model(\"lora\"),\n",
    "    \"Odd Layer\": create_student_model(\"odd\"),\n",
    "    \"Even Layer\": create_student_model(\"even\")\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.to(device)\n",
    "    results[name] = train_and_evaluate(model, name)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"| Model | Accuracy | F1 Score | Trainable Parameters |\")\n",
    "print(\"|-------|----------|----------|---------------------|\")\n",
    "for name, result in results.items():\n",
    "    trainable_params = sum(p.numel() for p in models[name].parameters() if p.requires_grad)\n",
    "    print(f\"| {name} | {result['eval_accuracy']:.4f} | {result['eval_f1']:.4f} | {trainable_params} |\")\n",
    "\n",
    "# Save the best model\n",
    "best_model = models[\"LoRA\"]\n",
    "best_model.save_pretrained(\"./best_lora_model\")\n",
    "tokenizer.save_pretrained(\"./best_lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=28, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=28, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from peft import PeftModel\n",
    "from collections import Counter\n",
    "\n",
    "# Load the trained LoRA model and tokenizer\n",
    "model_path = \"app/best_lora_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=28)\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GoEmotions dataset\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Emotion labels from GoEmotions\n",
    "emotion_labels = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\",\n",
    "    \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\",\n",
    "    \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\", \"pride\", \"realization\",\n",
    "    \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Single Example Inference:\n",
      "Text: I can't believe how amazing this day turned out!\n",
      "Predicted Emotion: surprise\n"
     ]
    }
   ],
   "source": [
    "# 1. Single Example Inference\n",
    "def predict_emotion(text):\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    \n",
    "    return emotion_labels[prediction]\n",
    "\n",
    "# Test single example\n",
    "sample_text = \"I can't believe how amazing this day turned out!\"\n",
    "print(\"\\n1. Single Example Inference:\")\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Predicted Emotion: {predict_emotion(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Batch Inference (5 random test samples):\n",
      "Text: It still wouldn't make sense. The problem isn't the looks\n",
      "True Emotion: disapproval\n",
      "Predicted Emotion: neutral\n",
      "\n",
      "Text: It's actually pretty cool in here today, not hot at all\n",
      "True Emotion: admiration\n",
      "Predicted Emotion: admiration\n",
      "\n",
      "Text: Funny part is There ISNT a wall (but maybe should be) and there ARE already plenty of \"Sensible\" Gun laws\n",
      "True Emotion: amusement\n",
      "Predicted Emotion: amusement\n",
      "\n",
      "Text: Hahaha so Iâ€™m no longer going to worry about it then lol\n",
      "True Emotion: amusement\n",
      "Predicted Emotion: amusement\n",
      "\n",
      "Text: So. Fucking. Tone. Deaf.\n",
      "True Emotion: annoyance\n",
      "Predicted Emotion: anger\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Batch Inference\n",
    "def batch_predict(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    return [emotion_labels[pred] for pred in predictions]\n",
    "\n",
    "# Test batch inference on 5 random test samples\n",
    "print(\"\\n2. Batch Inference (5 random test samples):\")\n",
    "random_samples = test_dataset.shuffle(seed=SEED).select(range(5))\n",
    "sample_texts = random_samples['text']\n",
    "true_labels = [emotion_labels[label[0]] if label else \"neutral\" for label in random_samples['labels']]\n",
    "predicted_labels = batch_predict(sample_texts)\n",
    "\n",
    "for text, true, pred in zip(sample_texts, true_labels, predicted_labels):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"True Emotion: {true}\")\n",
    "    print(f\"Predicted Emotion: {pred}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Emotion Distribution Analysis (100 samples):\n",
      "neutral: 0.430\n",
      "admiration: 0.130\n",
      "curiosity: 0.060\n",
      "gratitude: 0.060\n",
      "amusement: 0.050\n",
      "anger: 0.040\n",
      "joy: 0.030\n",
      "approval: 0.030\n",
      "surprise: 0.030\n",
      "confusion: 0.020\n",
      "love: 0.020\n",
      "excitement: 0.010\n",
      "caring: 0.010\n",
      "remorse: 0.010\n",
      "disappointment: 0.010\n",
      "disgust: 0.010\n",
      "annoyance: 0.010\n",
      "desire: 0.010\n",
      "optimism: 0.010\n",
      "fear: 0.010\n",
      "sadness: 0.010\n",
      "\n",
      "4. Error Analysis (first 5 errors from 50 samples):\n",
      "Text: It still wouldn't make sense. The problem isn't the looks\n",
      "True Emotion: disapproval\n",
      "Predicted Emotion: neutral\n",
      "\n",
      "Text: So. Fucking. Tone. Deaf.\n",
      "True Emotion: annoyance\n",
      "Predicted Emotion: anger\n",
      "\n",
      "Text: If you think all manufactured foods haven't got those contaminants then you're sadly mistaken.\n",
      "True Emotion: disapproval\n",
      "Predicted Emotion: neutral\n",
      "\n",
      "Text: Have nobody thought of the giant tech corporation selling overpriced, inferior products to sheep customers? That's odd.\n",
      "True Emotion: disapproval\n",
      "Predicted Emotion: curiosity\n",
      "\n",
      "Text: In what ways has [NAME] out-coached [NAME]? I see this all the time and yet never any examples.\n",
      "True Emotion: neutral\n",
      "Predicted Emotion: curiosity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Emotion Distribution Analysis\n",
    "def get_emotion_distribution(dataset, num_samples=100):\n",
    "    samples = dataset.shuffle(seed=SEED).select(range(num_samples))\n",
    "    texts = samples['text']\n",
    "    predictions = batch_predict(texts)\n",
    "    \n",
    "    dist = Counter(predictions)\n",
    "    total = sum(dist.values())\n",
    "    return {emotion: count/total for emotion, count in dist.items()}\n",
    "\n",
    "print(\"\\n3. Emotion Distribution Analysis (100 samples):\")\n",
    "dist = get_emotion_distribution(test_dataset)\n",
    "for emotion, freq in sorted(dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{emotion}: {freq:.3f}\")\n",
    "\n",
    "# 4. Error Analysis\n",
    "def error_analysis(dataset, num_samples=50):\n",
    "    samples = dataset.shuffle(seed=SEED).select(range(num_samples))\n",
    "    texts = samples['text']\n",
    "    true_labels = [label[0] if label else 0 for label in samples['labels']]\n",
    "    predicted_labels = [emotion_labels.index(pred) for pred in batch_predict(texts)]\n",
    "    \n",
    "    errors = []\n",
    "    for text, true, pred in zip(texts, true_labels, predicted_labels):\n",
    "        if true != pred:\n",
    "            errors.append({\n",
    "                'text': text,\n",
    "                'true': emotion_labels[true],\n",
    "                'predicted': emotion_labels[pred]\n",
    "            })\n",
    "    \n",
    "    return errors\n",
    "\n",
    "print(\"\\n4. Error Analysis (first 5 errors from 50 samples):\")\n",
    "errors = error_analysis(test_dataset)\n",
    "for error in errors[:5]:\n",
    "    print(f\"Text: {error['text']}\")\n",
    "    print(f\"True Emotion: {error['true']}\")\n",
    "    print(f\"Predicted Emotion: {error['predicted']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
